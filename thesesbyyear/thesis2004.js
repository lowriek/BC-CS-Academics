document.write('\
<div class="well">\
	<h3>2004 Honors Theses</h3>				\
	<div class="q">	\
		<h3 class="qhead"><a href="#q200401">Computation of Potentially Visible Set for Occluded Three-Dimensional Environments</a></h3>\
		<div class="answer" id="q200401">\
			<p><b>Author:</b> Derek Carr</p>\
			<p><b>Title:</b> <a href="pdf/04DerekCarr.pdf"><em> Computation of Potentially Visible Set for Occluded Three-Dimensional Environments</em></a></p>\
			<p><b>Advisor:</b> William Ames</p>\
			<p><b>Abstract</b></p>\
			<p><font size="4">This thesis deals with the problem of visibility culling in interactive three dimensional environments. Included in this thesis is a discussion surrounding the issues involved in both constructing and rendering three-dimensional environments. When rendering a three-dimensional environment, only a subset of objects in the environment is visible to the user. We refer to this subset of objects as the Potentially Visible Set (PVS). This thesis presents an algorithm that divides an environment into a network of convex cellular volumes connected by invisible portal regions. A renderer can then utilize this network of cells and portals to compute a PVS via a depth first traversal of the scene graph in real-time. Finally, this thesis discusses how a simulation engine might exploit this data structure to provide dynamic collision detection against the scene graph.</font></p>\
			<p><font size="4">Click <a href="attachments/04CarrThesisCode.zip">HERE</a> to download Derek Carr\'s code.</font></p>	\
		</div>   <!-- end answer -->\
	</div><!-- end question -->\
	<div class="q">	\
		<h3 class="qhead"><a href="#q200402">Construction of a Database of Annotated Natural Human Motion and its Application to Benchmarking</a></h3>\
		<div class="answer" id="q200402">\
			<p><b>Author:</b> Adam Chmielewski</p>\
			<p><b>Title:</b> <a href="pdf/04AdamChmielewski.pdf"><em> Construction of a Database of Annotated Natural Human Motion and its Application to Benchmarking</em></a></p>\
			<p><b>Advisor:</b> David Martin</p>\
			<p><b>Abstract</b></p>\
			<p><font size="4">Humans possess an uncanny ability to perceive the motion of other human beings. The performance of current algorithms on this task falls far short of human performance. In aiming to close this gap, we have collected video clips of natural human motion (e.g. walking, shopping, sports, etc.) and annotated figure tracks and head locations. We then use the annotated clips as a baseline to evaluate the quality of identification and tracking algorithms such as face detection and figure tracking. To date, we have developed annotation tools, collected and annotated 93 5-30 second clips, and evaluated two current face detection algorithms.</font></p>\
		</div>   <!-- end answer -->\
	</div><!-- end question -->	\
	<div class="q">	\
		<h3 class="qhead"><a href="#q200403">Face Based Indexing of Digital Photo Albums</a></h3>\
		<div class="answer" id="q200403">\
			<p><b>Author:</b> Matt Veino</p>\
			<p><b>Title:</b> <a href="pdf/04MattVeino.pdf"><em>Face Based Indexing of Digital Photo Albums</em></a></p>\
			<p><b>Advisor:</b> Sergio Alvarez</p>\
			<p><b>Abstract</b></p>\
			<p><font size="4">Managing a large collection of digital snapshots -- even for the average amateur digital photographer -- is a chore. Searching is often limited to captions, but writing a descriptive caption for each photo is overly tedious. We aim to simplify the management of a collection of photos by enabling searches on the content of the photographs themselves while minimizing the amount of supervision by the user. We focus on the problem of finding faces in photographs. The user identifies a small number of faces in the photo collection. Once we have these faces, we can run algorithms on other pictures to search through and find faces in the photographs. Once we have locations of faces, we will attempt to recognize the individuals.</font></p>\
			<p><font size="4">Click <a href="attachments/04VeinoThesisCode.zip">HERE</a> to download Matt Veino\'s code.</font></p>	\
		</div>   <!-- end answer -->\
	</div><!-- end question -->	\
</div>\
');

